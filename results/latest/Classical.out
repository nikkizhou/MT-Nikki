/work/nikki/MT/code/service.py:65: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'open-ended' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.
  df.loc[(df['Label'] == -1) & (df[category] > 0), 'Label'] = category
File 1 Combined: 
Label
option-posing     1384
open-ended         706
none-questions     544
suggestive          80
multiple             1
Name: count, dtype: int64
Label
1    2293
0    1390
3     577
2     544
Name: count, dtype: int64
Tuning model: Logistic Regression
Fitting 5 folds for each of 10 candidates, totalling 50 fits
Best parameters for LogisticRegression: {'solver': 'liblinear', 'C': 10}
Tuning model: SVM
Fitting 5 folds for each of 10 candidates, totalling 50 fits
Best parameters for SVC: {'kernel': 'rbf', 'gamma': 'scale', 'C': 10}
Tuning model: Random Forest
Fitting 5 folds for each of 10 candidates, totalling 50 fits
Best parameters for RandomForestClassifier: {'n_estimators': 50, 'min_samples_split': 5, 'max_depth': None}

Model: Logistic Regression
Accuracy: 0.7440
Classification Report:
                precision    recall  f1-score   support

    open-ended       0.72      0.70      0.71       278
 option-posing       0.75      0.80      0.78       459
none-questions       0.82      0.73      0.78       109
       leading       0.71      0.62      0.66       115

      accuracy                           0.74       961
     macro avg       0.75      0.71      0.73       961
  weighted avg       0.74      0.74      0.74       961

--------------------------------------------------

Model: SVM
Accuracy: 0.7680
Classification Report:
                precision    recall  f1-score   support

    open-ended       0.78      0.71      0.74       278
 option-posing       0.74      0.86      0.80       459
none-questions       0.88      0.69      0.77       109
       leading       0.79      0.61      0.69       115

      accuracy                           0.77       961
     macro avg       0.80      0.72      0.75       961
  weighted avg       0.77      0.77      0.77       961

--------------------------------------------------

Model: Random Forest
Accuracy: 0.7471
Classification Report:
                precision    recall  f1-score   support

    open-ended       0.85      0.61      0.71       278
 option-posing       0.69      0.93      0.79       459
none-questions       0.91      0.64      0.75       109
       leading       0.84      0.44      0.58       115

      accuracy                           0.75       961
     macro avg       0.82      0.66      0.71       961
  weighted avg       0.78      0.75      0.74       961

--------------------------------------------------

Model: Logistic Regression
Accuracies for each fold: [0.78251821 0.72944849 0.67429761 0.60208333 0.640625  ]
Average Accuracy: 0.6858
                precision    recall  f1-score   support

    open-ended       0.70      0.60      0.64      1390
 option-posing       0.66      0.81      0.73      2293
none-questions       0.90      0.61      0.73       544
       leading       0.63      0.49      0.55       576

      accuracy                           0.69      4803
     macro avg       0.72      0.63      0.66      4803
  weighted avg       0.70      0.69      0.68      4803

--------------------------------------------------

Model: SVM
Accuracies for each fold: [0.79396462 0.73257024 0.68366285 0.60416667 0.63958333]
Average Accuracy: 0.6908
                precision    recall  f1-score   support

    open-ended       0.69      0.61      0.65      1390
 option-posing       0.68      0.78      0.72      2293
none-questions       0.88      0.67      0.76       544
       leading       0.62      0.54      0.58       576

      accuracy                           0.69      4803
     macro avg       0.72      0.65      0.68      4803
  weighted avg       0.70      0.69      0.69      4803

--------------------------------------------------

Model: Random Forest
Accuracies for each fold: [0.80228928 0.76690947 0.67950052 0.61666667 0.596875  ]
Average Accuracy: 0.6924
                precision    recall  f1-score   support

    open-ended       0.75      0.52      0.62      1390
 option-posing       0.64      0.88      0.74      2293
none-questions       0.90      0.70      0.79       544
       leading       0.72      0.35      0.47       576

      accuracy                           0.69      4803
     macro avg       0.75      0.61      0.65      4803
  weighted avg       0.71      0.69      0.68      4803

--------------------------------------------------
