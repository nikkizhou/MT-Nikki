Total synthetic samples: 4368
Synthetic samples in training set: 4368
Label Count Training Set: 
Label
3    2183
2    2160
0    2028
1    1840
Name: count, dtype: int64
Label Count Test Set: 
Label
1    453
0    265
2    133
3    109
Name: count, dtype: int64
Map:   0%|          | 0/8211 [00:00<?, ? examples/s]Map:  12%|█▏        | 1000/8211 [00:00<00:01, 6716.23 examples/s]Map:  24%|██▍       | 2000/8211 [00:00<00:01, 4333.52 examples/s]Map:  49%|████▊     | 4000/8211 [00:00<00:00, 7177.46 examples/s]Map:  97%|█████████▋| 8000/8211 [00:00<00:00, 13041.76 examples/s]Map: 100%|██████████| 8211/8211 [00:00<00:00, 10386.97 examples/s]
Map:   0%|          | 0/960 [00:00<?, ? examples/s]Map: 100%|██████████| 960/960 [00:00<00:00, 17059.62 examples/s]
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Fold 1/3
Flattening the indices:   0%|          | 0/5474 [00:00<?, ? examples/s]Flattening the indices:  91%|█████████▏| 5000/5474 [00:00<00:00, 37969.25 examples/s]Flattening the indices: 100%|██████████| 5474/5474 [00:00<00:00, 37070.45 examples/s]
Flattening the indices:   0%|          | 0/2737 [00:00<?, ? examples/s]Flattening the indices: 100%|██████████| 2737/2737 [00:00<00:00, 45529.33 examples/s]
Accuracy: 0.9302

Classification Report:
                precision    recall  f1-score   support

    open-ended       0.92      0.96      0.94       676
 option-posing       0.90      0.85      0.88       614
none-questions       0.96      0.93      0.94       720
       leading       0.93      0.96      0.95       727

      accuracy                           0.93      2737
     macro avg       0.93      0.93      0.93      2737
  weighted avg       0.93      0.93      0.93      2737

Confusion matrix saved to /work/nikki/CM_BERT_Fold_0.png

Fold 2/3
Flattening the indices:   0%|          | 0/5474 [00:00<?, ? examples/s]Flattening the indices:  91%|█████████▏| 5000/5474 [00:00<00:00, 37827.76 examples/s]Flattening the indices: 100%|██████████| 5474/5474 [00:00<00:00, 36956.84 examples/s]
Flattening the indices:   0%|          | 0/2737 [00:00<?, ? examples/s]Flattening the indices: 100%|██████████| 2737/2737 [00:00<00:00, 46715.65 examples/s]
Early stopping triggered at epoch 3
Accuracy: 0.9843

Classification Report:
                precision    recall  f1-score   support

    open-ended       1.00      0.98      0.99       676
 option-posing       0.97      0.99      0.98       613
none-questions       0.98      1.00      0.99       720
       leading       1.00      0.98      0.99       728

      accuracy                           0.98      2737
     macro avg       0.98      0.98      0.98      2737
  weighted avg       0.98      0.98      0.98      2737

Confusion matrix saved to /work/nikki/CM_BERT_Fold_1.png

Fold 3/3
Flattening the indices:   0%|          | 0/5474 [00:00<?, ? examples/s]Flattening the indices:  91%|█████████▏| 5000/5474 [00:00<00:00, 37282.37 examples/s]Flattening the indices: 100%|██████████| 5474/5474 [00:00<00:00, 36402.47 examples/s]
Flattening the indices:   0%|          | 0/2737 [00:00<?, ? examples/s]Flattening the indices: 100%|██████████| 2737/2737 [00:00<00:00, 45870.62 examples/s]
Accuracy: 0.9960

Classification Report:
                precision    recall  f1-score   support

    open-ended       0.99      0.99      0.99       676
 option-posing       0.99      0.99      0.99       613
none-questions       1.00      1.00      1.00       720
       leading       1.00      1.00      1.00       728

      accuracy                           1.00      2737
     macro avg       1.00      1.00      1.00      2737
  weighted avg       1.00      1.00      1.00      2737

Confusion matrix saved to /work/nikki/CM_BERT_Fold_2.png

Average Accuracy across all folds: 0.9702
