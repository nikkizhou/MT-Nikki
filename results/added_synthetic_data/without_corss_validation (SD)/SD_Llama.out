Total synthetic samples: 4368
Synthetic samples in training set: 4368
Label Count Training Set: 
Label
3    2183
2    2160
0    2028
1    1840
Name: count, dtype: int64
Label Count Test Set: 
Label
1    453
0    265
2    133
3    109
Name: count, dtype: int64
Map:   0%|          | 0/8211 [00:00<?, ? examples/s]Map:  12%|█▏        | 1000/8211 [00:00<00:00, 7753.50 examples/s]Map:  37%|███▋      | 3000/8211 [00:00<00:00, 10091.93 examples/s]Map:  49%|████▊     | 4000/8211 [00:00<00:00, 6444.99 examples/s] Map:  97%|█████████▋| 8000/8211 [00:00<00:00, 12943.58 examples/s]Map: 100%|██████████| 8211/8211 [00:00<00:00, 11244.43 examples/s]
Map:   0%|          | 0/960 [00:00<?, ? examples/s]Map: 100%|██████████| 960/960 [00:00<00:00, 20832.53 examples/s]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Accuracy: 0.8625

Classification Report:
                precision    recall  f1-score   support

    open-ended       0.86      0.94      0.90       265
 option-posing       0.84      0.92      0.88       453
none-questions       0.93      0.68      0.78       133
       leading       0.96      0.66      0.78       109

      accuracy                           0.86       960
     macro avg       0.90      0.80      0.83       960
  weighted avg       0.87      0.86      0.86       960

Confusion matrix saved to /work/nikki/CM_Llama.png
