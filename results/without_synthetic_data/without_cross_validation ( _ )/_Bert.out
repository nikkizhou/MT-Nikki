Label Count Training Set: 
Label
1    1840
0    1125
3     467
2     411
Name: count, dtype: int64
Label Count Test Set: 
Label
1    453
0    265
2    133
3    109
Name: count, dtype: int64

Map:   0%|          | 0/3843 [00:00<?, ? examples/s]
Map:  26%|██▌       | 1000/3843 [00:00<00:00, 3240.36 examples/s]
Map:  52%|█████▏    | 2000/3843 [00:00<00:00, 2648.65 examples/s]
Map:  78%|███████▊  | 3000/3843 [00:00<00:00, 3624.57 examples/s]
Map: 100%|██████████| 3843/3843 [00:00<00:00, 3902.96 examples/s]

Map:   0%|          | 0/960 [00:00<?, ? examples/s]
Map: 100%|██████████| 960/960 [00:00<00:00, 8492.09 examples/s]
Map: 100%|██████████| 960/960 [00:00<00:00, 8303.91 examples/s]
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Accuracy: 0.9260
Validation Classification Report Bert:
                precision    recall  f1-score   support

    open-ended       0.90      0.95      0.93       265
 option-posing       0.95      0.92      0.94       453
none-questions       0.90      0.92      0.91       133
       leading       0.91      0.88      0.90       109

      accuracy                           0.93       960
     macro avg       0.92      0.92      0.92       960
  weighted avg       0.93      0.93      0.93       960

Confusion matrix saved to /work/nikki/CM_BERT.png
