Synthetic data label count: 
Label
2    1749
3    1716
0     903
Name: count, dtype: int64
Total synthetic samples: 4368
Synthetic samples in training set: 4368
Label Count Training Set: 
Label
3    2183
2    2160
0    2028
1    1840
Name: count, dtype: int64
Label Count Test Set: 
Label
1    453
0    265
2    133
3    109
Name: count, dtype: int64
Map:   0%|          | 0/8211 [00:00<?, ? examples/s]Map:  12%|█▏        | 1000/8211 [00:00<00:01, 5302.67 examples/s]Map:  24%|██▍       | 2000/8211 [00:00<00:01, 5088.29 examples/s]Map:  49%|████▊     | 4000/8211 [00:00<00:00, 7713.40 examples/s]Map:  85%|████████▌ | 7000/8211 [00:00<00:00, 10507.94 examples/s]Map: 100%|██████████| 8211/8211 [00:00<00:00, 9341.94 examples/s] 
Map:   0%|          | 0/960 [00:00<?, ? examples/s]Map: 100%|██████████| 960/960 [00:00<00:00, 14994.05 examples/s]
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Accuracy: 0.8750
Total time taken (training + evaluation): 102.52 seconds
GPU memory used: 863.67 MB
Validation Classification Report :
                precision    recall  f1-score   support

    open-ended       0.91      0.87      0.89       265
 option-posing       0.85      0.93      0.89       453
none-questions       0.87      0.78      0.83       133
       leading       0.91      0.77      0.84       109

      accuracy                           0.88       960
     macro avg       0.89      0.84      0.86       960
  weighted avg       0.88      0.88      0.87       960

Confusion matrix saved to /work/nikki/CM_BERT.png
